% !TeX root = main.tex
\input{template.tex}

\begin{document}



\section{Sequence annotation}

\paragraph{1. Example: Dishonest Casino}
Situation: Casino plays dice.  Usually, a fair dice (\(F\)) is used, but
occasionally it is replaced by a loaded dice (\(L\)).  We have the observables
(alphabet) and a sequence:
\[
    \Alphabet := \{ 1, 2, 3, 4, 5, 6 \} \qquad
    X = (1,3,6,6,5,...)
\]
Our task is, given a sequence \(X\) of observables, to find when the loaded and
the fair dice was used. The strategy we employ consists of two steps:
\begin{enumerate}[label=\arabic*.]
\item Capture the known features of how casino operates.  We capture this in
 a probabilistic model.
\item Use the model from the first step to analyze input sequences X and tell
 us, ie.\@ predict if and when the loaded dice was used to generate sequence
 X.
\end{enumerate}

Let's mark down the key features of our casino:
\begin{enumerate}[label=(\alph*)]
\item The dice \(F\) generates \(a\in\Alphabet\) with equal probability:
 \[
   e_F(1) = e_F(2) = \dotsb = 1/6
 \]
 Here \(e_F(a)\) with \(a\in\Alphabet\) denotes the emission probability
 of state \(F\) to emit observable \(a\in\Alphabet\).
\item The loaded dice \(L\) has a bias:
 \[
   e_L(1) = e_L(2) = e_L(3) = e_L(4) = 1/10 \qquad e_L(5) = e_L(6) = 3/10
 \]
 Analogously, \(e_L(a)\) denotes the emission probability of state \(L\).
 \item On average, a change from loaded to fair dice is made after \(20\)
 runs.  If we denote \(t(L,F)\) the transition probability to switch from state
 \(L\) to \(F\) we have:
 \[
   t(L,F) = \frac{1}{20}
 \]
 \item We have the reverse transition probability of
 \[
   t(F,L) = \frac{1}{100}
 \]
 Ie.\@ on average, a switch from \(F\) to \(L\) occurs after \(100\) runs.
 \item The casino has no bias as to whether it sarts with \(L\) or \(F\).
\end{enumerate}

\subparagraph{Some definitions}

We consider the sequence of observables \(X=(x_1,x_2,\dotsc,x_l)\) where \(l\)
is the length of \(X\) or \(|X|\).
Based on \(X\), we cannot conclude whether \(x_i\) was generated by the fair
state \(F\) or loaded state \(L\).
Potential annotations of \(X\) are:
\[
    H_x=(F,F,\dotsc,F) \qquad \widetilde{H}_x=(F,F,\dotsc,L,\dotsc,F)
\]
For this casino, any sequence \(X\) of observables of length \(l\)
corresponds to \(2^l\) potential annotations.
Our question: Which is the \enquote{best} annotation of a given \(X\)?

\begin{figure}
\centering
\begin{tikzpicture}[every node/.style={ellipse,draw},>=latex,node distance=2cm]

    \node[] (start) {Start};
    \node[draw,above right of=start] (F) {F};
    \node[draw,below right of=start] (L) {L};
    \node[draw,below right of=F] (end) {End};
    \draw[->] (F) edge [loop above] (F);
    \draw[->] (L) edge [loop below] (L);
    \draw[->] (start) -- (F);
    \draw[->] (start) -- (L);
    \draw[->] (F) edge [bend left] (L);
    \draw[->] (L) edge [bend left] (F);
    \draw[->] (F) -- (end);
    \draw[->] (L) -- (end);

    
\end{tikzpicture}
\end{figure}
Here the start and end state of the model are \enquote{silent}, that is they do
not emit observables.

Let's specify the rest of the model.  We have equal chance to start with either
\(L\) or \(F\) and have the transitions between \(F\) to \(L\) and vice-versa.
As there's (1) no bias as to average length of any sequence \(X\) is \(100\)
observables.
\[
t(\text{Start},F) = t(\text{Start},L) = \frac{1}{2}
t(F,L) = \frac{1}{100}
t(L,F)=\frac{1}{20}
t(F,\text{End}) = t(L,\text{End}) = \frac{1}{100}
\]
The ramining probabilities in the model can be inferred, as for any state \(s\)
in the model, we know that when being based in any state \(s\) in our model, the
sum of transition probabilities to any other state in the model (including \(s\)
itself) has to be \(1\).
\[
  \sum t(s,s') = 1
\]
So we have the chance for staying in the fair dice:
\[
  t(F,F) = 1 - t(F,l) - t(F,\text{End})
         = 1 - \frac{1}{100} - \frac{1}{100}
         = \frac{98}{100}
\]
Analogously for the loaded dice an a bit reduced probability:
\[
  t(L,L) = 1 - t(L,F) - t(L,\text{End})
         = 1 - \frac{5}{100} - \frac{1}{100}
         = \frac{94}{100}
\]
Conclusion: Given all the transition probabilities and the emission
probabilities of all non-silent states, we have a fully parametrized
probabilistic model.

The mode of the dis-honest asino can be defined as follows:
\begin{itemize}
\item set of states \(\States=\{\text{Start},F,L,\text{End}\}\) of states,
  of which two are silent.
\item set of transition probabilities
  \(\Trans = \{ t(a,b) \mid a,b\in\States \}\)
\item set of emission probabilities
  \(\EmProb = \{ e_s(a) \mid s\in\States, a\in\Alphabet, s\text{ non-silent} \}\)
\item set of observables \(\Alphabet = {1,2,\dotsc,6}\)
\end{itemize}

Note that the transition and emission probabilities need to fulfill the
following constraints:
\begin{itemize}
\item For all fixed \(\forall a\in\States\) the sum of probabilities to
  transition into any \(b\in\States\) state must be \(\sum t(a,b) = 1\)
\item The emission probabilities within each non-silent state \(a\) need to add
  up to \(1\) as well
  \(\sum e_a(i) = 1\)
\end{itemize}

Lets consider the example where our sequence is \(X=(1,5,6,3,2)\).  We want to
find a state path, eg.\@ \(\Path(X)=(\text{Start},F,L,L,F,F,\text{End})\).  The
overall probability of a a given state path \(\Path\) and input sequence \(X\)
in the model is:
\[
    P(X,\Path) = t(\text{Start},F) \cdot
                 e_F(1) \cdot t(F,L) \cdot
                 e_L(5) \cdot t(L,L) \cdot
                 e_L(6) \cdot t(L,F) \cdot
                 e_F(3) \cdot t(F,F) \cdot
                 e_F(2) \cdot t(F,\text{End})
\]

This model is called a Hidden Markov Model, \enquote{hidden} because all
observables can be produced by all states, that is the actual states are
hidden by the sequence.

\subsection{The Viterbi algorithm}

Setting: Given a so-called Hidden Markvo Model as defined by the four sets
\(\States,\Alphabet,\Trans,\EmProb\) as introduced earlier and a sequence \(X\)
of observables from \(\Alphabet\) of length \(L := |X|\), we want to identify the
state path \(\Path(X)\) in the model, which maximizes \(P(X,\Path)\), ie.\@
the optimal state path or Viterbi path for a given \(X\) and model:
\[
    \Path^* := \argmax_{\Path} \{ P(X,\Path) \}
\]
Our question: Can we optimize \(P(X,\Path)\) as we go along the input sequence?

The main idea is to use dynamic programming.  That is, suppose we have
determined \(\Path^*\) up to a sequence position \(j\), eg.\@ \((x_1,\dotsc,x_j)\)
with \(1\leq j\leq L\), and we know the state in which this state path ends on
the state \(i\):
\[
    \Path^*(j,i) = (\Path_0^*, \Path_1^*, \dotsc, \Path_j^* = i)
                 = \argmax_{\Path} \{ P(x_j,\Path) \mid \Path_j = i \}
\]
Then we can easily derive the \enquote{complete} optimal path \(\Path^*(j+1,k)\)
with the state \(k\in\States\) for sequence \((x1,\dotsc,x_j,x_{j+1})\).
Because the best overall probability for a sequence \((x_1,\dotsc,x_{j+1})\)
ending in state \(k\in\States\) is equal to the maximum probability of any
optimal path to any \(m\) optimal sequences and then further to \(k\), times the
emission probability for \(k\).
\[
    P(x_{j+1}, \Path^*(j+1,k))
  = \max_{m\in\States} \{ P(x_j,\Path^*(j,m)) \cdot t(m,k) \} \cdot e_k(x_{j+1})
\]
So, the probability of the best state path finishing in state \(k\) at sequence
position \(j+1\) can be derived from the best state path\emph{s} finishing at
the previous sequence position \(j\) in any state \(m\in\States\), from the
transition probability of going from state \(m\) to state \(k\) and from the
emission probability probability of reading \(x_{j+1}\) in state \(k\).

This is the key idea behind the Viterbi algorithm.

\begin{definition}[Viterbi Algorithm]
Let \(X=(x_1,\dotsc,x_L)\) a sequence of length \(L\) and given a model \(H\)
with \(\States=\{0,1,\dotsc,N\}\) where \(0\) is the Start state and \(N\) is
the End state, and where these two states are the only silent states in the
model.  Also with alphabet \(\Alphabet\)
\[
    \Trans = \{ t(a,b) \mid a,b \in \States \} \qquad
    \EmProb = \{ e_a(x) \mid a \in \States, x\in\Alphabet \}
\]
We note down \(v_k(i) = P(x_i,\Path^*(i,k))\) as the Viterbi element for
sequence position \(i\) and state \(k\) in the model.

\begin{enumerate}[label=(\alph*)]
\item Initialisation. Purpose: Ensure all state paths start in Start state
  \(s=0\):
  \[
      v_k(0) = \begin{cases}
          1, & k = 0 \text{ (ie. Start state)} \\
          0, & k \neq 0, k\in\States
      \end{cases}
  \]
\item Recursion.  Purpose: Build up the next optimal paths step-by-step, walking
through the found optimal paths of the previous step.
See algorithm~\ref{alg:viterbi}.



\AlgoDisplayBlockMarkers\SetAlgoNoLine%
\begin{algorithm}[ht]
\For{$i = 1$; $i \leq L$; $i++$} {
    \For{$k = 1$; $k < N$; $k++$} {
        $v_k(i) = \max\limits_{m\in\States} \{ v_m(i-1) \cdot t(m,k) \} \cdot e_k(x_i)$\;
        $\pointer_k(i) = \argmax\limits_{m\in\States} \{ v_m(i-1) \cdot t(m,k) \}$\;
    }
}
\label{alg:viterbi}
\caption{Viterbi-Algorithm: Recursion step}
\end{algorithm}
\item Termination. Purpose: Ensure all state paths considered end in the End
  state at the end of the input sequence \(X\):
  \[
      v_N(L) = \max_{m\in\States} \{ v_m(L) \cdot t(m,N) \}
  \]
  Where the pointer is:
  \[
      \pointer_N(L) = \argmax_{m\in\States} \{ v_m(L) \cdot t(m,N) \}
  \]
\end{enumerate}
What we are \emph{actually} are interested in is the best annotation for input
sequence \(X\), ie.\@ the Viterbi path \(\Path^*\).

\[
\begin{blockarray}{ccccccc}
\begin{block}{c[cccccc]}
N      &        & \hdots & x &               &        &  \pointer_N(L) \\
       & \vdots &        & x &               &        &             \\
k      &        &        & X & \pointer_k(i) &        &  \\
       & \vdots &        & x &               &        &  \\
       &        &        & x &               &        &  \\
0      & -1     &        & x &               &        &  \\
\end{block}
       & 0      & \hdots & i-1 & i & \hdots & L
\end{blockarray}
\qquad
\begin{blockarray}{ccccccc}
\begin{block}{c[cccccc]}
N      & 0      &        &  &        &        &  v_N(L) \\
       &        &        &  &        &        &         \\
k      & \vdots &        &  & v_k(i) &        &  \\
       &        &        &  &        &        &  \\
       & 0      &        &  &        &        &  \\
0      & 1      &        &  &        &        &  \\
\end{block}
       & 0      & \hdots & i-1 & i & \hdots & L
\end{blockarray}
\]
%TODO: Pointer Matrix and Viterbi Matrix
%This is just a placeholder for tomorrow.



In order to recover \(\Path^*\), we need a trace back procedure and the pointer
matrix:
The idea is to recover the optiimal path by going from the end sequence and
state path to the start of sequence and state path.
We start the traceback in \(v_N(L)\) in End state \(N\) and least sequence
position \(L\), set
\[
    \Path^* = (N) = (\Path^*_{L+1})
\]
We determine the state at the previous sequence position using the pointers:
\[
    \Path^*_{j-1} = \pointer_{\Path^*_j} (j)
\]
We continue with the recursion while \(\pointer\neq -1\) and get optimal state:
\[
    \Path^* = (\Path^*_0, \Path^*_1,\dotsc,\Path^*_{L+1})
\]
\end{definition}

\subsection*{Exercises}
\begin{enumerate}[label=(\alph*)]
\item The memory requirement for calculating the entire Viterbi matrix is
  given by the number of items in the matrix.  As it is a \(L\times N\)-matrix,
  this is asymptotically in \(\mathcal{O}(N\cdot L)\).

  The time used includes an outer loop of \(L\) steps, with an inner loop of
  \(N-1\) steps with a calculation of \(\max\) over \(N\) states, yielding an
  asymptotic time of \(\mathcal{O}(L\cdot N^2)\).
\item Given the Viterbi matrix, the traceback takes \(\mathcal{O}(L)\) in terms
  of space and memory.
\item We can improve time by knowing that we only need to iterate over
  \(N_\text{out}\) many nodes in the inner loop, while the maximum now does not
  consider the whole set \(S\) but a subset of cardinality \(N_\text{in}\).
  Thus we have a better time of \(\mathcal{O}(N_\text{out}N_\text{in}\cdot L)\).

  Space is not improved, however, as we don't know \emph{which} subset of states
  are our input/output states.
\item From a space requirement perspective, we only need to hold the previous
  column of the Viterbi matrix, thus reducing our memory complexity to
  \(\mathcal{N}\).  In regards of time, we can't remove any calculation.
\item Since it doesn't necessarily hold that \(t(a,b) = t(b,a)\) the reverse
  path may have a different probability.
\item We are given only the Viterbi matrix, ie.\@ the emission probabilities,
  the \enquote{current} Viterbi element (going back from the end) and all
  previous Viterbi elements.  We can then transform the equation from our
  algorithm and find the previous element that then fulfills the equation:
  \[
    v_k(i) = \max_{m\in\States} \{ v_m(i-1) \cdot t(m,k) \} \cdot e_k(x_i)
    \Leftrightarrow
    \frac{v_k(i)}{e_k(x_i)} = \max_{m\in\States} \{ v_m(i-1) \cdot t(m,k) \}
  \]
\item We only save one of the maximum elements -- which one is up to the
  implementation -- so we end up with the probability of that path (which is
  the same as any other optimal, by definition).
\end{enumerate}

\section{RNA Structure Prediction}

So far, we've seen that a Hidden Markov Model (HMM) can do, given an input
sequence \(X\), predict the best annotation using the CYK-Algorithm to yield
\(P(X,\Path^*)\) and the path/annotation \(\Path^*\) itself.

We will now see how one can use an HMM to
\begin{enumerate}[label=(\alph*)]
\item test competing hypotheses, eg.\@ does \(X\) derive only from the loaded
  dice or the fair dice?
\item tell us what the most likely state label or annotation for a sequence
  position \(x_i\) is.
\end{enumerate}

However, we HMMs limit us, as they analyze a sequence \(X\) in a uni-directional
way, ie.\@ cannot be used to model long-range correlations, eg.\@ the base-pairs
of RNA secondary structure.  That is, correlations between base-pairs that
aren't directly adjacent to each other cannot be modeled, as we have no
\enquote{memory} of the pre-previous sequence element.

\subsection{Context-free grammars}

Idea: Model RNA structures as sentences of a so-called language where symbols
may appear in pairs.


We can use a grammar to generate sequences belonging to a language.  Eg.\@
\enquote{This dog eats fish} is a sentence belonging to the English grammar. So
is \enquote{Green ideas sleep wonderfully}.
\begin{definition}[Transformation Grammar]
A transformation grammar consists of
\begin{itemize}
\item a set of symbols \(\Syms\) which can
  be sub-divided into two \emph{disjoint} sub-sets of \emph{terminal symbols}
  (or Alphabet) \(\Alphabet\) -- it corresponds to the symbols in the input
  sequences -- and a set of non-terminal symbols \(\NTSyms\), so:
  \[
    \Syms=\Alphabet\sqcup\NTSyms
  \]
\item a set of production rules \(\Rules\) where every element, ie.\@ every
  production rule, has the form
  \[
    \alpha \to \beta
  \]
  where \(\alpha\) and \(\beta\) are each a string of symbols from \(Q\) and
  \(\alpha\) contains at least one non-terminal symbols form \(\NTSyms\). 
\end{itemize}
\end{definition}

By convention we use lower-case letters for elements in \(\Alphabet\) and
upper-case letters for non-terminals in \(\NTSyms\), and every sequence of
production rules has to start with a non-terminal \(S\in\NTSyms\).
Also we define the following symbols with a fixed meaning:
\begin{itemize}
\item A non-terminal symbol (word) \(W\in\NTSyms\)
\item A terminal symbol \(a\in\Alphabet\)
\item Strings \(\alpha,\gamma\) of symbols from \(\Syms\) \emph{including} the
  empty string
\item Similarly the string \(\beta\) \emph{excluding} the empty string.
\item The empty string \(\Empty\) itself.
\end{itemize}

\begin{definition}[Context-free Grammar]
A context-free grammar is a transformational grammar where all production rules
in \(\Rules\) have the form
\[
  W \to \alpha
\]
\end{definition}

A few examples of \emph{transformational} grammars:
\begin{enumerate}[label=(\arabic*)]
\item \(\Syms=\Alphabet\sqcup\NTSyms\), where the alphabet \(\Alphabet=\{a,b\}\)
  and \(\NTSyms=\{T,S,\Empty\}\) with the rules
  \[
    \Rules=\{ S\to aT, S\to bS, T\to aS, T\to bt, T\to \Empty \}
  \]
  The rules can also be summarized as such:
  \[
    S\to aT \mid bS \qquad T\to aS \mid bT \mid \Empty
  \]

  This grammar can be used to generate sentences of \(a\)'s and \(b\)'s with an
  odd number of \(a\)'s.  Remember: Every sequence of production rules starts
  with \(S\).  For example the sentence \(ababa\) (only symbols from
  \(\Alphabet\) allowed here!).

  We can generate this sentence as follows, using the rules 1, 4, 3, 2, 1 and
  finally 5, by starting with \(S\):
  \[
    S\to aT \to abT \to abaS \to ababS \to ababaT \to ababa
  \]
\item \(\Syms=\Alphabet\sqcup\NTSyms\) where \(\Alphabet=\{a,b\}\) and
  \(\NTSyms=\{S\}\) with the rules
  \[
    \Rules=\{ S\to aSa, S\to bSb, S\to aa, S\to bb \}
  \]
  which again can be written as:
  \[
    S\to aSa \mid bSb \mid aa \mid bb
  \]
  Note: This is a context-free grammar.

  The sentence \(aabaabaa\) can be generated as follows with the rules
  1, 1, 2, 3:
  \[
    S\to aSa \to aaSaa \to aabSbaa \to aabaabaa
  \]
  Here we can make terminals appear in pairs which is just what we need to
  capture base-pairs in RNA structures.  Hidden Markov Models only allow for
  \emph{regular grammars}.
\end{enumerate}

In nature, we know that some RNA structures are pseudo-knotted, ie.\@ they
contain at least two base-pair \(\{i,j\}\) and \(\{i',j'\}\) where
\begin{alignat*}{3}
  x &i < &i' < &j < &j' \\
  x &(   &[    &)   &]
\end{alignat*}
Note, pseudo-knotted RNA structures cannot be (easily) captured using
context-free grammars (in general, this would only be possible to capture using
recursively enumerable languages, ie.\@ the decision problem being NP-hard,
however the pseudo-knots occuring in nature are easier to tackle).  We will
ignore them from now.

Our strategy using CFGs (context-free grammars) to predict RNA secondary
structures:
\begin{enumerate}[label=\arabic*.]
\item define a CFG whose production rules can handle base-pairs as well as
  structural features of known RNA structures.
\item assign probabilities (emission \& transition probabilities as it will
  turn out) to the production rules of the grammar.  This will allow us to
  assign an overall probability to a given input sequence \(X\) and a series of
  production rules that predict the structural annotation for \(X\).
\item Come up with an efficient algorithm to retrieve the sequence of
  production rules yielding the overall highest probability for a given input
  sequence \(X\) (this will be similar to the Viterbi algorithm for HMMs).
\end{enumerate}

Let's look at another example, the Pfold grammar (Knudsen \& hein,
Bioinformatics 1999, 15(6)):
We have a context-free grammar with the symbols \(\Syms=\Alphabet\sqcup\NTSyms\)
and \(\Alphabet=\{s,d\}\) and \(\NTSyms=\{S,L,F\}\) and the rules:
\begin{align}
  S &\to LS \\
  S &\to L \\
  F &\to dFd \\
  F &\to LS \\
  L &\to s \\
  L &\to dFd
\end{align}
Given the humble RNA structure as sence \(ssddssdds\), we can generate it as
shown in Figure~\ref{fig:ast}.

\begin{figure}[ht]
\centering
\Tree [.S [.L s ] [.S [.L s ] [.S [.L d [.F d [.F [.L s ] [.S [.L s ]  ] ] d ]  d ] [.S [.L s ] ] ] ] ]
\label{fig:ast}
\caption{Derivation Tree example}
\end{figure}

Our strategy for turning the CFG of Pfold into a probabilistic model is to
create a Stochastic CFG.  The idea is to assign transition probabilities to
the different choices given the same non-terminal on the left side of the rule,
ie.\@ these three pairs of alternatives:
\begin{align}
  S &\to LS \mid L
  F &\to dFd \mid LS
  L &\to s \mid dFd
\end{align}
When dealing with RNA sequences, we would set \(\Alphabet=\{a,c,g,u\}\)
(the RNA nucleotides).  Emission probabilities will be assigned to any
gproduction rule with terminals on the right hand side.  Eg.\@ for \(L\to dFd\),
we would assign the emission probability \(e_d\) for that case:
\[
  e_d(x,y) = \begin{cases}
      >0, &\text{if \((x,y)\) is a consensus base pair} \\
      0, &\text{else}
  \end{cases}
\]
This yields a matrix of probabilities with \(0\) values for invalid base-pairs:
\begin{table}[H]
\centering
\begin{tabular}{l|cccc}
    $e_d$ & $a$ & $c$ & $g$ & $u$ \\
    \hline 
    $a$   & 0   & 0   & 0   & *   \\
    $c$   & 0   & 0   & *   & 0   \\
    $g$   & 0   & *   & 0   & *   \\
    $u$   & *   & 0   & *   & 0
\end{tabular}
\end{table}

\subsection{Stochastic context-free grammars (SCFGs)}

We assume in the following a context-free grammar where all production rules in
\(\Rules\) are specified in so-called Chomsky normal formal, ie.\@ all
production rules either have the form
\begin{enumerate}[label=(\alph*)]
\item\label{rule-type-a} (or bifurcation) \(A\to BC\) (we say that \(B\) is the
  left child-state and \(C\) the right child-state of \(A\)), or
\item\label{rule-type-b} \(A\to a\),
\end{enumerate}
where \(A,B,C\in\NTSyms\) and \(a\in\Alphabet\).  We then
\begin{enumerate}[label=\arabic*.]
\item assign transition probabilities \(t_A(B,C)\in[0,1]\) to any production
  rule of type~\ref{rule-type-a},
\item assign emission probabilities \(e_A(a)\in[0,1]\) to any production rule of
  type~\ref{rule-type-b} and
\item require that for any non-terminal \(A\in\NTSyms\) holds:
  \[
    \sum_{A\to BC\in\Rules} t_A(B,C) + \sum_{A\to a\in\Rules} e_A(a) = 1
  \]
\end{enumerate}

A good way to remember/appreciate this constraint is to order the production
rules in \(\Rules\) first by \(A\in\States\) (what's on the left) and, second,
by type~\ref{rule-type-a}, then type~\ref{rule-type-b}:
\begin{align*}
&\left.
\begin{array}{cc}
  \left.
  \begin{array}{cc}
    A &\to BC \\
    A &\to DE \\
    A &\to FG \\
      &\vdots \\
  \end{array}\right\}& \text{ transition probabilities} \\
  \left.
  \begin{array}{cc}
    A &\to a \\
    A &\to b \\
      &\vdots
  \end{array}\right\}& \text{ emission probabilities} \\
\end{array}\right\} \sum = 1 \\
%--------------------
&\quad\begin{array}{cc}
  B &\to AC \\
    &\vdots
\end{array}
\end{align*}

We will call \(A\in\States\) the states of the model.  States \(A\in\States\)
without any production rules of type~\ref{rule-type-b}, ie.\@ \(A\to a\), are
called silent rules.

\paragraph{Writing a CFG in Chomsky normal form:}
For A CFG, the production rules have the form \(W\to\beta\).  In order to write
optimisation algorithms for any SCFG in a compound manner, it is good to
rephrase the production rules in Chomsky normal form CNF), ie.\@ to force the
production rules to have on of the two formats specified earlier.

This conversion can always be achieved by replacing any production rule of type
\(W\to\beta\) by one or more production rules in CNF and by introducing
\emph{new} states to \(\States\), if needed.

For example, the production rule \(S\to aSa\) can be written in CNF as follows,
with the new states \(W_1,W_2\in\States\):
\begin{align}
  S   &\to W_1 W_2 \\
  W_1 &\to a \\
  W_2 &\to SW_1
\end{align}
From now on, assume any SCFG is specified in CNF\@.

We want a strategy for deriving the optimal derivation tree for given sequence
\(X\) and SCFG\@.  Note, we need to operate in a similar way as the Viterbi, but
now on the space of derivation trees rather than linear state paths, and, again
we can express \(P(X,\Path)\), \(\Path\) being the derivation tree, as product
of the respective emission and transition probabilities.  Our idea is to
optimize once again in a step-wise fashion, similar to the Viterbi algorithm.

Our key idea is to note down the optimal probability \(\gamma(i,j,A)\) of the
best derivation tree from sub-sequence \((x_i,\dotsc,x_j)\) with \(1\leq i\),
\(j\leq L\) and \(i<j\) and \(A\in\States\).  We realize, we can derive
\(\gamma(i,j,A)\), if we already know \emph{all} probabilities \(\gamma(i,k,B)\)
and \(\gamma(k+1,j,C)\) for all in-between sequence positions
\(k\in\{i,\dotsc,j-1\}\) and all possible pairs of child states \(BC\) of \(A\),
ie.\@ \(A\to BC\), \(A\to B'C'\), etc\@.

Suppose we have a derivation tree of a top-node \(A\) for the sequence
\((x_i,\dotsc,x_j)\), where \(A\) can derive to \(BC\), \(B'C'\) or \(B''C''\),
where in turn \(B\) can derive to the terminal sequence \((x_i,\dotsc,x_k)\) and
\(C\) to \((x_{k+1},\dotsc,x_j)\) with coressponding probabilities
\(\gamma(i,k,B)\) and \(\gamma(k+1,j,C)\).  Then \(A\to BC\) is one possible
generation for the sequence and its derivation (and the subderivation of \(B\),
\(C\)) is the thing we record now (cf.\@ the Viterbi elements in the previous
column in the Viterbi matrix).  We want to optimize this derivation in order
to \emph{get the optimal derivation for the given sequence with the top element
\(A\)} and its optimal probability \(\gamma(i,j,A)\).

While for Viterbi, we walked along the matrix to the right, here we
\enquote{work up} the derivation tree search space.

\paragraph{The Cocke-Jonnge-Kasami (CYK) algorithm:}  Our goal is to, given an
input sequence \(X\) with symbols from \(\Alphabet\) and an SCFG in CNF\@,
device the derivation tree \(\Path^*\) (ie.\@ so-called optimal derivation tree)
which maximizes \(P(X,\Path)\) in the model.

We will define the following convention similar to the previous for the Viterbi
algorithm:
\begin{itemize}
\item SCFG as specified by \(Q=\States\sqcup\Alphabet\) and the
  production rules \(\Rules\), where \(\States=\{0,\dotsc,N\}\) with Start \(0\)
  as the state with which each posssible derivation tree has to start and the
  transition probabilities as well as the emission probabilities
  \begin{align*}
    \Trans &=\{t_A(B,C) \mid A,B,C\in\States\} \\
    \EmProb&=\{e_A(x) \mid x\in\Alphabet, A\in\States, A\text{ non-silent} \}
  \end{align*}
\item Let \(\gamma(i,j,A)\) be the probability of the
  best derivation tree covering the sub-sequence \((x_i,\dotsc,x_j)\),
  \(i\leq j\), with top tree-node \(A\).
\item We use \(\tau(i,j,A):=(B,C,K)\) to describe the pointer for
  \(\gamma(i,j,A)\), where \(A\to BC\in\Rules\) and \(k\in\{i,\dotsc,j-1\}\).
\end{itemize}

The algorithm is as follows:
\begin{enumerate}[label=(\alph*)]
\item Initialisation.  Purpose: Read symbols from input sequence using
  production rules of type~\ref{rule-type-b}, ie.\@ consider all so-called leaf
  nodes of all potential derivation trees.  Refer to
  algorithm~\ref{alg:cyk-init} for a formal description.

\AlgoDisplayBlockMarkers\SetAlgoNoLine%
\begin{algorithm}[ht]
\For{$i = 1$; $i \leq L$; $i++$} {
    \For{$A = 0$; $A \leq N$; $A++$} {
        $y(i,i,A) = \begin{cases} e_a(x_i), &\text{if \(A\) not silent}\\ 0, &\text{else} \end{cases}$ \;
        \tau(i,i,A) = (0,0,0) \tcc*{no previous derivation trees at this point}
    }
}
\label{alg:cyk-init}
\caption{CYK-Algorithm: Initialisation}
\end{algorithm}

\item Recursion.  We want to calculate the probabilities for all sub-trees
  first, and then the transition probability to \enquote{build} the complete
  tree from its subtrees (cf.\@ Algorithm~\ref{alg:cyk-rec}).  Note we don't
  require a termination step at the end recursion, we have
  \(\gamma(1,L,0)=P(X,\Path^*)\), ie.\@ the probability of the best derivation
  tree.

\AlgoDisplayBlockMarkers\SetAlgoNoLine%
\begin{algorithm}[ht]
\tcc{loop over all meaningful $(i,j)$ pairs in a meaningful order to ensure
the left sub-tree and right sub-tree are calculated ahead of the overall
transition probability.}
\For{$i = L-1$; $i \geq 1$; $i--$} {
    \For{$j = i+1$; $i \leq L$; $j++$} {
        \For{$A = 0$; $A \leq N$; $A++$} {
            \tcc{calculate left sub-tree, right-subtree, and overall}
            $\gamma(i,j,A) = \max\limits_{\substack{B,C\\A\to BC\in\States}}
                \left\{ \max\limits_{k\in\{i,\dotsc,j-1}
                \left\{ \gamma(i,k,B) \cdot \gamma(k+1,j,C) \cdot t_A(B,C) \right\} \right\} $\;
            $\tau(i,j,A) = (\widetilde{B},\widetilde{C},\widetilde{K})$
            \tcc*{winners from above}
        }
    }
}
\label{alg:cyk-rec}
\caption{CYK-Algorithm: Recursion step}
\end{algorithm}

\end{enumerate}

We visualize the CYK using the CYK matrix which contains the \(\gamma(i,j,A)\)
elements, needed for optimization.  In the initialization step, the diagonal
elements from the lower-left to the upper-right corner are calculated.
Afterwards, the upper triangle is calculated in the recursion, moving in the
columsn with \(\gamma(i,k,B)\) and rows with \(\gamma(k+1,j,C)\).

\newcommand{\udots}{\reflectbox{$\ddots$}}
\[
\begin{blockarray}{ccccccc}
\begin{block}{c[cccccc]}
X      & \gamma(1,2,0) &        &   &               &        & * \\
       &        &        &   &               & \udots &   \\
j      &        & \hdots & \gamma(i,j,A)  &               &        &  \\
       &        &        &   &               &        &  \\
       &        & \udots & \vdots &               &        &  \\
0      & *      &        &   &               &        &  \\
\end{block}
       & 0      & \hdots & i &               & \hdots & X
\end{blockarray}
\]

\paragraph{Traceback algorithm for CYK:}  Our task is to derive the best
derivation tree \(\Path^*\) for a given SCFG and \(X\), ie.\@
\[
  \Path^* = \argmax_{\Path} \{ P(X,\Path^*) \} = \gamma(1,L,0)
\]
The idea is again to use a pointer matrix to derive the tree \(\Path^*\).
In the Initialisation step, we push the tuple \((1,L,0)\) on the stack.
In the recursion, we consider \((i,j,A):=\tau(1,L,0)\) and compare
\(\tau(i,j,A) = (0,0,0)\).  If this is equal, we attach the sequence symbol
\(x_i\) as child of \(A\) as we have reached the leaf node of the derivation
tree.  Otherwise we set \(\tau(i,j,A)=(B,C,K)\), ie.\@ set states \(B\) and
\(C\) as child states of \(A\) to derivation tree, that is push \((k+1, j, C)\)
and \((i,k,B)\).

\paragraph{Visual comparison of CYK \& Viterbi}

\begin{itemize}
\item On the one hand we have the 3-dimensional CYK-matrix with two elements
  per \(x\) and one for states \(\States\).

  For Viterbi, we have a two-dimensional matrix, as we only have one element
  per \(x\).
\item In the Initialisation step, we we read all symbols from \(X\) using
  non-silent states \(A\in\States\) and production rules of type \(A\to a\),
  with \(a\in\Alphabet\) and \(A\in\States\) only (ie.\@ fill the diagonal in
  the matrix).

  Whereas in the Viterbi algorithm's Initialisation step, we don't read symbols
  from \(X\) but ensure that all considered state paths start in Start state
  \(S=0\) (ie.\@ fill the leftmost column in the matrix).
\item When recursing, we read no symbols from \(X\) anymore but move from leaf
  nodes to the top node of all potential trees (\(S=0\)), discarding sub-optimal
  sub-trees.  We consider bifurcating rules \(A\to BC\) only.  This means, we
  fill the upper triangle of the matrix, starting from the upper right corner
  and moving left and then up from the diagonal.

  In the Viterbi algorithm, we read a sequence from start to end, while
  investigating state paths up to the respective position in \(X\) and
  discarding sub-optimal state paths.  In the matrix, we move right and then
  up, filling the columns from the left.
\item In CYK we do not have any termination step, at the end of the recursion
  we have \(P(X,\Path^*)=\gamma(1,L,0)\).

  Viterbi needs an additional step after having read all of the sequence, as we
  need to ensure that the optimal state path \(\Path^*\) finished in the end
  state.
\item We start the traceback with the top node in the upper left corner
  \((1,L,0)\) where the \(0\) means Start state (top to bottom).  We then have
  bifurcations moving down and to the right, each bifurcation having further
  bifurcations til we reach the diagonal of terminal symbols and we've completed
  the derivation tree.  Note, \(S=0\) may not only appear as the top node of
  \(\Path^*\) but also in other places.

  Viterbi is tracedback in reverse, that is we start with the End state of
  \(\Path^*\) in the upper right corner, hopping from pointer to pointer til
  we reach the Start state in the lower left.
\end{itemize}

\paragraph{Interpreting the results of probabilistic models (eg.\@ HMMs, SCFGs):}

\begin{itemize}
\item In Hidden Markov Models and SCFGs, \(P(X,\Path)\) denotes the probability
  of the state path \(\Path\)~/ derivation tree in the model for the input
  sequence \(X\), respectively.
\item The path/tree \(\Path\) can be converted into an annotation of \(X\).
\item We can describe the probability of a path/tree given a sequence using the
  definition of conditional probability as a fraction of the intersection
  of the probabilties by the probability of the sequence itself -- which then
  can be calculated as the sum of all the intersected probabilities of all
  state paths~/ derivation trees with a fixed sequence.
  \[
    P(\Path \mid X) = \frac{P(X,\Path)}{P(X)} \qquad
    P(X) = \sum_{\Path} P(X,\Path)
  \]
\item This means we can calculate \(P(X)\) using the forward algorithm in the
  case of Viterbi, ie.\@ replace the max-operation in the algorithm by
  summation.

  For SCFGs can be calculated by the so-called inside algorithm, where we
  replace \emph{the two max operations} in CYK by corresponding summation
  signs.
\item It only holds that \(P(\Path\mid X) = P(\text{annotation} \mid X)\) if
  the annotation corresponds to \emph{a unique} state path or
  derivation tree in the model.  In this case the HMM/SCFG is called
  unambigious, ie.\@ if \emph{any} potential annotation \(A\) of \(X\)
  corresponds to \emph{at most} one state path~/ derivation tree \(\Path\) in
  the model.

  If we want to calculate the formula in the unambigious case, we need to  to
  look at all \(\widetilde{\Path}\) that correspond to the same annotation of
  \(X\):
  \[
    P(A\mid X) = \begin{cases}
        P(\Path \mid X), &\text{model unambigious} \\
        \sum\limits_{\widetilde{\Path}} P(\widetilde{\Path} \mid X), &\text{otherwise}
    \end{cases}
  \]
\end{itemize}

This means, if dealing with an ambigious model, \(\Path^*\) \emph{does not}
have to correspond to the \emph{best} annotation of any \(X\).

\end{document}
