% !TeX root = main.tex
\input{template.tex}

\begin{document}



\section{Sequence annotation}

\paragraph{1. Example: Dishonest Casino}
Situation: Casino plays dice.  Usually, a fair dice (\(F\)) is used, but
occasionally it is replaced by a loaded dice (\(L\)).  We have the observables
and a sequence:
\[
    \Observ := \{ 1, 2, 3, 4, 5, 6 \} \qquad
    X = (1,3,6,6,5,...)
\]
Our Task is, given a sequence \(X\) of observables, to find when the loaded and
the fair dice was used.

Our strategy consists of two steps:
\begin{enumerate}[label=Step \arabic*.]
\item Capture the known features of how casino operates.  We capture this in
 a probabilistic model.
\item Use the model from the first step to analyze input sequences X and tell
 us, ie.\@ predict if and when the loaded dice was used to generate sequence
 X.
\end{enumerate}

Let's mark down the key features of our casino:
\begin{enumerate}[label=(\alph*)]
\item The dice \(F\) generates \(a\in\Observ\) with equal probability:
 \[
   e_F(1) = e_F(2) = \dotsb = 1/6
 \]
 Here \(e_F(a)\) with \(a\in\Observ\) denotes the emission probability
 of state \(F\) to emit observable \(a\in\Observ\).
\item The loaded dice \(L\) has a bias:
 \[
   e_L(1) = e_L(2) = e_L(3) = e_L(4) = 1/10 \qquad e_L(5) = e_L(6) = 3/10
 \]
 Analogously, \(e_L(a)\) denotes the emission probability of state \(L\).
 \item On average, a change from loaded to fair dice is made after \(20\)
 runs.  If we denote \(t(L,F)\) the transition probability to switch from state
 \(L\) to \(F\) we have:
 \[
   t(L,F) = \frac{1}{20}
 \]
 \item We have the reverse transition probability of
 \[
   t(F,L) = \frac{1}{100}
 \]
 Ie.\@ on average, a switch from \(F\) to \(L\) occurs after \(100\) runs.
 \item The casino has no bias as to whether it sarts with \(L\) or \(F\).
\end{enumerate}

\subparagraph{Some definitions}

We consider the sequence of observables \(X=(x_1,x_2,\dotsc,x_l)\) where \(l\)
is the length of \(X\) or \(|X|\).
Based on \(X\), we cannot conclude whether \(x_i\) was generated by the fair
state \(F\) or loaded state \(L\).
Potential annotations of \(X\) are:
\[
    H_x=(F,F,\dotsc,F) \qquad \widetilde{H}_x=(F,F,\dotsc,L,\dotsc,F)
\]
For this casino, any sequence \(X\) of observables of length \(l\)
corresponds to \(2^l\) potential annotations.
Our question: Which is the \enquote{best} annotation of a given \(X\)?

\begin{figure}
\centering
\begin{tikzpicture}[every node/.style={ellipse,draw},>=latex,node distance=2cm]

    \node[] (start) {Start};
    \node[draw,above right of=start] (F) {F};
    \node[draw,below right of=start] (L) {L};
    \node[draw,below right of=F] (end) {End};
    \draw[->] (F) edge [loop above] (F);
    \draw[->] (L) edge [loop below] (L);
    \draw[->] (start) -- (F);
    \draw[->] (start) -- (L);
    \draw[->] (F) edge [bend left] (L);
    \draw[->] (L) edge [bend left] (F);
    \draw[->] (F) -- (end);
    \draw[->] (L) -- (end);

    
\end{tikzpicture}
\end{figure}
Here the start and end state of the model are \enquote{silent}, that is they do
not emit observables.

Let's specify the rest of the model.  We have equal chance to start with either
\(L\) or \(F\) and have the transitions between \(F\) to \(L\) and vice-versa.
As there's (1) no bias as to average length of any sequence \(X\) is \(100\)
observables.
\[
t(\text{Start},F) = t(\text{Start},L) = \frac{1}{2}
t(F,L) = \frac{1}{100}
t(L,F)=\frac{1}{20}
t(F,End) = t(L,\text{End}) = \frac{1}{100}
\]
The ramining probabilities in the model can be inferred, as for any state \(s\)
in the model, we know that when being based in any state \(s\) in our model, the
sum of transition probabilities to any other state in the model (including \(s\)
itself) has to be \(1\).
\[
  \sum t(s,s') = 1
\]
So we have the chance for staying in the fair dice:
\[
  t(F,F) = 1 - t(F,l) - t(F,\text{End})
         = 1 - \frac{1}{100} - \frac{1}{100}
         = \frac{98}{100}
\]
Analogously for the loaded dice an a bit reduced probability:
\[
  t(L,L) = 1 - t(L,F) - t(L,\text{End})
         = 1 - \frac{5}{100} - \frac{1}{100}
         = \frac{94}{100}
\]
Conclusion: Given all the transition probabilities and the emission
probabilities of all non-silent states, we have a fully parametrized
probabilistic model.

The mode of the dis-honest asino can be defined as follows:
\begin{itemize}
\item set of states \(\States=\{\text{Start},F,L,\text{End}\}\) of states,
  of which two are silent.
\item set of transition probabilities
  \(\Trans = \{ t(a,b) \mid a,b\in\States \}\)
\item set of emission probabilities
  \(\EmProb = \{ e_s(a) \mid s\in\States, a\in\Observ, s\text{ non-silent} \}\)
\item a set of observables \(\Observ = {1,2,\dotsc,6}\)
\end{itemize}

Note that the transition and emission probabilities need to fulfill the
following constraints:
\begin{itemize}
\item For all fixed \(\forall a\in\States\) the sum of probabilities to
  transition into any \(b\in\States\) state must be \(\sum t(a,b) = 1\)
\item The emission probabilities within each non-silent state \(a\) need to add
  up to \(1\) as well
  \(\sum e_a(i) = 1\)
\end{itemize}

Lets consider the example where our sequence is \(X=(1,5,6,3,2)\).  We want to
find a state path, eg.\@ \(\Path(X)=(\text{Start},F,L,L,F,F,\text{End})\).  The
overall probability of a a given state path \(\Path\) and input sequence \(X\)
in the model is:
\[
    P(X,\Path) = t(\text{Start},F) \cdot
                 e_F(1) \cdot t(F,L) \cdot
                 e_L(5) \cdot t(L,L) \cdot
                 e_L(6) \cdot t(L,F) \cdot
                 e_F(3) \cdot t(F,F) \cdot
                 e_F(2) \cdot t(F,\text{End})
\]

This model is called a Hidden Markov Model, \enquote{hidden} because all
observables can be produced by all states, that is the actual states are
hidden by the sequence.

\subsection{The Viterbi algorithm}

Setting: Given a so-called Hidden Markvo Model as defined by the four sets
\(\States,\Observ,\Trans,\EmProb\) as introduced earlier and a sequence \(X\)
of observables from \(\Observ\) of length \(L := |X|\), we want to identify the
state path \(\Path(X)\) in the model, which maximizes \(P(X,\Path)\), ie.\@
the optimal state path or Viterbi path for a given \(X\) and model:
\[
    \Path^* := \argmax_{\Path} \{ P(X,\Path) \}
\]
Our question: Can we optimize \(P(X,\Path)\) as we go along the input sequence?

The main idea is to use dynamic programming.  That is, suppose we have
determined \(\Path^*\) up to a sequence position \(j\), eg.\@ \((x_1,\dotsc,x_j)\)
with \(1\leq j\leq L\), and we know the state in which this state path ends on
the state \(i\):
\[
    \Path^*(j,i) = (\Path_0^*, \Path_1^*, \dotsc, \Path_j^* = i)
                 = \argmax_{\Path} \{ P(X_j,\Path) \mid \Path_j = i) \}
\]
Then we can easily derive the \enquote{complete} optimal path \(\Path^*(j+1,k)\)
with the state \(k\in\States\) for sequence \((X_1,\dotsc,X_j,X_{j+1})\).
Because the best overall probability for a sequence \((X_1,\dotsc,X_{j+1})\)
ending in state \(k\in\States\) is equal to the maximum probability of any
optimal path to any \(m\) optimal sequences and then further to \(k\), times the
emission probability for \(k\).
\[
    P(X_{j+1}, \Path^*(j+1,k))
  = \max_{m\in\States} \{ P(X_j,\Path^*(j,m)) \cdot t(m,k) \} \cdot e_k(X_{j+1})
\]
So, the probability of the best state path finishing in state \(k\) at sequence
position \(j+1\) can be derived from the best state path\emph{s} finishing at
the previous sequence position \(j\) in any state \(m\in\States\), from the
transition probability of going from state \(m\) to state \(k\) and from the
emission probability probability of reading \(X_{j+1}\) in state \(k\).

This is the key idea behind the Viterbi algorithm.

\begin{definition}[Viterbi Algorithm]
Let \(X=(X_1,\dotsc,X_L)\) a sequence of length \(L\) and given a model \(H\)
with \(\States=\{0,1,\dotsc,N\}\) where \(0\) is the Start state and \(N\) is
the End state, and where these two states are the only silent states in the
model.  Also with alphabet \(\Observ\)
\[
    \Trans = \{ t(a,b) \mid a,b \in States \} \qquad
    \EmProb = \{ e_a(x) \mid a \in \States, x\in\Observ \}
\]
We note down \(v_k(i) = P(X_i,\Path^*(i,k))\) as the Vierbi element for
sequence position \(i\) and state \(k\) in the model.

\begin{enumerate}[label=(\alph*)]
\item Initialisation. Purpose: Ensure all state paths start in Start state
  \(s=0\):
  \[
      v_k(0) = \begin{cases}
          1, & k = 0 \text{ (ie. Start state)} \\
          0, & k \neq 0, k\in\States
      \end{cases}
  \]
\item Recursion.

\SetStartEndCondition{ (}{)}{)}
\SetAlgoBlockMarkers{}{\}}
\SetKwFor{For}{for}{ \{}{}
\SetKwIF{If}{ElseIf}{Else}{if}{ \{}{elif}{else \{}{}
\SetKwFor{While}{while}{ \{}{}
\AlgoDisplayBlockMarkers\SetAlgoNoLine%
\begin{algorithm}
\For{$i = 1$; $i \leq L$; $i++$} {
    \For{$k = 1$; $k < N$; $k++$} {
        $t_k(i) = \max\limits_{m\in\States} \{ v_m(i-1) \cdot t(m,k) \} \cdot e_k(X_i)$\;
        $\pointer_k(i) = \argmax\limits_{m\in\States} \{ v_m(i-1) \cdot t(m,k) \}$\;
    }
}
\end{algorithm}
\item Termination. Purpose: Ensure all state paths considered end in the End
  state at the end of the input sequence \(X\):
  \[
      v_N(L) = \max_{m\in\States} \{ v_m(L) \cdot t(m,N) \}
  \]
  Where the pointer is:
  \[
      \pointer_N(L) = \argmax_{m\in\States} \{ v_m(L) \cdot t(m,N) \}
  \]
\end{enumerate}
What we are \emph{actually} are interested in is the best annotation for input
sequence \(X\), ie.\@ the Viterbi path \(\Path^*\).

%TODO: Pointer Matrix and Viterbi Matrix



In order to recover \(\Path^*\), we need a trace back procedure and the pointer
matrix:
The idea is to recover the optiimal path by going from the end sequence and
state path to the start of sequence and state path.
We start the traceback in \(v_N(L)\) in End state \(N\) and least sequence
position \(L\), set
\[
    \Path^* = (N) = (\Path^*_{L+1})
\]
We determine the state at the previous sequence position using the pointers:
\[
    \Path^*_{j-1} = \pointer_{\Path^*_j} (j)
\]
We continue with the recursion while \(\pointer\neq -1\) and get optimal state:
\[
    \Path^* = (\Path^*_0, \Path^*_1,\dotsc,\Path^*_{L+1})
\]
\end{definition}

\end{document}
